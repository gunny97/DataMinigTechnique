{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keonwoo/anaconda3/envs/bgmRS/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import argparse\n",
    "from kobert_tokenizer import KoBertTokenizer\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances, paired_euclidean_distances, paired_manhattan_distances\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "import math\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from numpy import ndarray\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import Tensor, device\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "from typing import List, Dict, Tuple, Type, Union\n",
    "from kobert_tokenizer import KoBertTokenizer\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s -   %(message)s', datefmt='%m/%d/%Y %H:%M:%S',\n",
    "                    level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLM_MODEL(object):\n",
    "    \"\"\"\n",
    "    A class for embedding sentences, calculating similarities, and retriving sentences by DiffCSE. The code here is provided by SimCSE.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name_or_path: str, \n",
    "                device: str = None,\n",
    "                num_cells: int = 100,\n",
    "                num_cells_in_search: int = 10,\n",
    "                pooler = None):\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)\n",
    "        # self.tokenizer = KoBertTokenizer.from_pretrained(model_name_or_path)\n",
    "        self.model = AutoModel.from_pretrained(model_name_or_path)\n",
    "        if device is None:\n",
    "            device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = device\n",
    "\n",
    "        self.index = None\n",
    "        self.is_faiss_index = False\n",
    "        self.num_cells = num_cells\n",
    "        self.num_cells_in_search = num_cells_in_search\n",
    "\n",
    "        if pooler is not None:\n",
    "            self.pooler = pooler\n",
    "        else:\n",
    "            logger.info(\"Use `cls_before_pooler` for DiffCSE models. If you want to use other pooling policy, specify `pooler` argument.\")\n",
    "            self.pooler = \"cls_before_pooler\"\n",
    "    \n",
    "    def encode(self, sentence: Union[str, List[str]], \n",
    "                device: str = None, \n",
    "                return_numpy: bool = False,\n",
    "                normalize_to_unit: bool = True,\n",
    "                keepdim: bool = False,\n",
    "                batch_size: int = 2,\n",
    "                max_length: int = 128) -> Union[ndarray, Tensor]:\n",
    "\n",
    "        target_device = self.device if device is None else device\n",
    "        self.model = self.model.to(target_device)\n",
    "        \n",
    "        single_sentence = False\n",
    "        if isinstance(sentence, str):\n",
    "            sentence = [sentence]\n",
    "            single_sentence = True\n",
    "\n",
    "        embedding_list = [] \n",
    "        with torch.no_grad():\n",
    "            total_batch = len(sentence) // batch_size + (1 if len(sentence) % batch_size > 0 else 0)\n",
    "            for batch_id in tqdm(range(total_batch)):\n",
    "                inputs = self.tokenizer(\n",
    "                    sentence[batch_id*batch_size:(batch_id+1)*batch_size], \n",
    "                    padding=True, \n",
    "                    truncation=True, \n",
    "                    max_length=max_length, \n",
    "                    return_tensors=\"pt\"\n",
    "                )\n",
    "                inputs = {k: v.to(target_device) for k, v in inputs.items()}\n",
    "                outputs = self.model(**inputs, return_dict=True)\n",
    "                if self.pooler == \"cls\":\n",
    "                    embeddings = outputs.pooler_output\n",
    "                elif self.pooler == \"cls_before_pooler\":\n",
    "                    embeddings = outputs.last_hidden_state[:, 0]\n",
    "                else:\n",
    "                    raise NotImplementedError\n",
    "                if normalize_to_unit:\n",
    "                    embeddings = embeddings / embeddings.norm(dim=1, keepdim=True)\n",
    "                embedding_list.append(embeddings.cpu())\n",
    "        embeddings = torch.cat(embedding_list, 0)\n",
    "        \n",
    "        if single_sentence and not keepdim:\n",
    "            embeddings = embeddings[0]\n",
    "        \n",
    "        if return_numpy and not isinstance(embeddings, ndarray):\n",
    "            return embeddings.numpy()\n",
    "        return embeddings\n",
    "    \n",
    "    def similarity(self, queries: Union[str, List[str]], \n",
    "                    keys: Union[str, List[str], ndarray], \n",
    "                    device: str = None) -> Union[float, ndarray]:\n",
    "        \n",
    "        query_vecs = self.encode(queries, device=device, return_numpy=True) # suppose N queries\n",
    "        \n",
    "        if not isinstance(keys, ndarray):\n",
    "            key_vecs = self.encode(keys, device=device, return_numpy=True) # suppose M keys\n",
    "        else:\n",
    "            key_vecs = keys\n",
    "\n",
    "        # check whether N == 1 or M == 1\n",
    "        single_query, single_key = len(query_vecs.shape) == 1, len(key_vecs.shape) == 1 \n",
    "        if single_query:\n",
    "            query_vecs = query_vecs.reshape(1, -1)\n",
    "        if single_key:\n",
    "            key_vecs = key_vecs.reshape(1, -1)\n",
    "        \n",
    "        # returns an N*M similarity array\n",
    "        similarities = cosine_similarity(query_vecs, key_vecs)\n",
    "        \n",
    "        if single_query:\n",
    "            similarities = similarities[0]\n",
    "            if single_key:\n",
    "                similarities = float(similarities[0])\n",
    "        \n",
    "        return similarities\n",
    "    \n",
    "    def build_index(self, sentences_or_file_path: Union[str, List[str]], \n",
    "                        use_faiss: bool = None,\n",
    "                        faiss_fast: bool = False,\n",
    "                        device: str = None,\n",
    "                        batch_size: int = 64):\n",
    "\n",
    "        if use_faiss is None or use_faiss:\n",
    "            try:\n",
    "                import faiss\n",
    "                assert hasattr(faiss, \"IndexFlatIP\")\n",
    "                use_faiss = True \n",
    "            except:\n",
    "                logger.warning(\"Fail to import faiss. If you want to use faiss, install faiss through PyPI. Now the program continues with brute force search.\")\n",
    "                use_faiss = False\n",
    "        \n",
    "        # if the input sentence is a string, we assume it's the path of file that stores various sentences\n",
    "        if isinstance(sentences_or_file_path, str):\n",
    "            sentences = []\n",
    "            with open(sentences_or_file_path, \"r\") as f:\n",
    "                logging.info(\"Loading sentences from %s ...\" % (sentences_or_file_path))\n",
    "                for line in tqdm(f):\n",
    "                    sentences.append(line.rstrip())\n",
    "            sentences_or_file_path = sentences\n",
    "        \n",
    "        logger.info(\"Encoding embeddings for sentences...\")\n",
    "        embeddings = self.encode(sentences_or_file_path, device=device, batch_size=batch_size, normalize_to_unit=True, return_numpy=True)\n",
    "\n",
    "        logger.info(\"Building index...\")\n",
    "        self.index = {\"sentences\": sentences_or_file_path}\n",
    "        \n",
    "        if use_faiss:\n",
    "            quantizer = faiss.IndexFlatIP(embeddings.shape[1])  \n",
    "            if faiss_fast:\n",
    "                index = faiss.IndexIVFFlat(quantizer, embeddings.shape[1], min(self.num_cells, len(sentences_or_file_path))) \n",
    "            else:\n",
    "                index = quantizer\n",
    "\n",
    "            if (self.device == \"cuda\" and device != \"cpu\") or device == \"cuda\":\n",
    "                if hasattr(faiss, \"StandardGpuResources\"):\n",
    "                    logger.info(\"Use GPU-version faiss\")\n",
    "                    res = faiss.StandardGpuResources()\n",
    "                    res.setTempMemory(20 * 1024 * 1024 * 1024)\n",
    "                    index = faiss.index_cpu_to_gpu(res, 0, index)\n",
    "                else:\n",
    "                    logger.info(\"Use CPU-version faiss\")\n",
    "            else: \n",
    "                logger.info(\"Use CPU-version faiss\")\n",
    "\n",
    "            if faiss_fast:            \n",
    "                index.train(embeddings.astype(np.float32))\n",
    "            index.add(embeddings.astype(np.float32))\n",
    "            index.nprobe = min(self.num_cells_in_search, len(sentences_or_file_path))\n",
    "            self.is_faiss_index = True\n",
    "        else:\n",
    "            index = embeddings\n",
    "            self.is_faiss_index = False\n",
    "        self.index[\"index\"] = index\n",
    "        logger.info(\"Finished\")\n",
    "    \n",
    "    def search(self, queries: Union[str, List[str]], \n",
    "                device: str = None, \n",
    "                threshold: float = 0,\n",
    "                top_k: int = 5) -> Union[List[Tuple[str, float]], List[List[Tuple[str, float]]]]:\n",
    "        \n",
    "        if not self.is_faiss_index:\n",
    "            if isinstance(queries, list):\n",
    "                combined_results = []\n",
    "                for query in queries:\n",
    "                    results = self.search(query, device)\n",
    "                    combined_results.append(results)\n",
    "                return combined_results\n",
    "            \n",
    "            similarities = self.similarity(queries, self.index[\"index\"]).tolist()\n",
    "            id_and_score = []\n",
    "            for i, s in enumerate(similarities):\n",
    "                if s >= threshold:\n",
    "                    id_and_score.append((i, s))\n",
    "            id_and_score = sorted(id_and_score, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "            results = [(self.index[\"sentences\"][idx], score) for idx, score in id_and_score]\n",
    "            return results\n",
    "        else:\n",
    "            query_vecs = self.encode(queries, device=device, normalize_to_unit=True, keepdim=True, return_numpy=True)\n",
    "\n",
    "            distance, idx = self.index[\"index\"].search(query_vecs.astype(np.float32), top_k)\n",
    "            \n",
    "            def pack_single_result(dist, idx):\n",
    "                results = [(self.index[\"sentences\"][i], s) for i, s in zip(idx, dist) if s >= threshold]\n",
    "                return results\n",
    "            \n",
    "            if isinstance(queries, list):\n",
    "                combined_results = []\n",
    "                for i in range(len(queries)):\n",
    "                    results = pack_single_result(distance[i], idx[i])\n",
    "                    combined_results.append(results)\n",
    "                return combined_results\n",
    "            else:\n",
    "                return pack_single_result(distance[0], idx[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x,y):\n",
    "  \"\"\" return euclidean distance between two lists \"\"\"\n",
    " \n",
    "  return math.sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))\n",
    "\n",
    "def squared_sum(x):\n",
    "  \"\"\" return 3 rounded square rooted value \"\"\"\n",
    " \n",
    "  return round(math.sqrt(sum([a*a for a in x])),3)\n",
    "  \n",
    "def cos_similarity(x,y):\n",
    "  \"\"\" return cosine similarity between two lists \"\"\"\n",
    " \n",
    "  numerator = sum(a*b for a,b in zip(x,y))\n",
    "  denominator = squared_sum(x)*squared_sum(y)\n",
    "  return round(numerator/float(denominator),3)\n",
    "\n",
    "def evaluation(eval_dataset,model):\n",
    "    sen_emb1 = model.encode(eval_dataset['sentence1'].tolist())\n",
    "    sen_emb2 = model.encode(eval_dataset['sentence2'].tolist())\n",
    "    labels = eval_dataset['score']\n",
    "\n",
    "    cosine_scores = 1 - (paired_cosine_distances(sen_emb1, sen_emb2))\n",
    "    manhattan_distances = -paired_manhattan_distances(sen_emb1, sen_emb2)\n",
    "    euclidean_distances = -paired_euclidean_distances(sen_emb1, sen_emb2)\n",
    "    dot_products = [np.dot(emb1, emb2) for emb1, emb2 in zip(sen_emb1, sen_emb2)]\n",
    "    \n",
    "    eval_pearson_cosine, _ = pearsonr(labels, cosine_scores)\n",
    "    eval_spearman_cosine, _ = spearmanr(labels, cosine_scores)\n",
    "\n",
    "    eval_pearson_manhattan, _ = pearsonr(labels, manhattan_distances)\n",
    "    eval_spearman_manhattan, _ = spearmanr(labels, manhattan_distances)\n",
    "\n",
    "    eval_pearson_euclidean, _ = pearsonr(labels, euclidean_distances)\n",
    "    eval_spearman_euclidean, _ = spearmanr(labels, euclidean_distances)\n",
    "\n",
    "    eval_pearson_dot, _ = pearsonr(labels, dot_products)\n",
    "    eval_spearman_dot, _ = spearmanr(labels, dot_products)\n",
    "\n",
    "    score = {'eval_pearson_cosine': eval_pearson_cosine,\n",
    "            'eval_spearman_cosine': eval_spearman_cosine,\n",
    "            'eval_pearson_manhattan': eval_pearson_manhattan,\n",
    "            'eval_spearman_manhattan': eval_spearman_manhattan,\n",
    "            'eval_pearson_euclidean': eval_pearson_euclidean,\n",
    "            'eval_spearman_euclidean': eval_spearman_euclidean,\n",
    "            'eval_pearson_dot': eval_pearson_dot,\n",
    "            'eval_spearman_dot': eval_spearman_dot}\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ko_sbert_multitask = 'jhgan/ko-sbert-multitask'\n",
    "msbert = 'sentence-transformers/paraphrase-xlm-r-multilingual-v1'\n",
    "kcelectra = \"beomi/KcELECTRA-base\"\n",
    "kosimcse = \"BM-K/KoSimCSE-roberta\"\n",
    "kobert = 'monologg/kobert'\n",
    "kodiffcse = \"/home/keonwoo/anaconda3/envs/KoDiffCSE/sroberta_change_lr\"\n",
    "koroberta = \"klue/roberta-base\"\n",
    "\n",
    "path = \"/home/keonwoo/anaconda3/envs/KoDiffCSE/data/ko_sts_test.txt\"\n",
    "\n",
    "data = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "koroberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "08/17/2022 20:56:24 - INFO - __main__ -   Use `cls_before_pooler` for DiffCSE models. If you want to use other pooling policy, specify `pooler` argument.\n",
      "100%|██████████| 688/688 [00:13<00:00, 50.03it/s]\n",
      "100%|██████████| 688/688 [00:12<00:00, 55.52it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_pearson_cosine': 0.25679844613822966,\n",
       " 'eval_spearman_cosine': 0.322253040444909,\n",
       " 'eval_pearson_manhattan': 0.30918020338289237,\n",
       " 'eval_spearman_manhattan': 0.3230152408737565,\n",
       " 'eval_pearson_euclidean': 0.3081440348576169,\n",
       " 'eval_spearman_euclidean': 0.3222551174547771,\n",
       " 'eval_pearson_dot': 0.2567991498556915,\n",
       " 'eval_spearman_dot': 0.3222567713268123}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "koroberta= PLM_MODEL(koroberta)\n",
    "evaluation(data, koroberta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ko_sbert_multitask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/17/2022 19:52:40 - INFO - __main__ -   Use `cls_before_pooler` for DiffCSE models. If you want to use other pooling policy, specify `pooler` argument.\n",
      "100%|██████████| 688/688 [00:13<00:00, 49.79it/s]\n",
      "100%|██████████| 688/688 [00:12<00:00, 55.47it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_pearson_cosine': 0.8282746047533913,\n",
       " 'eval_spearman_cosine': 0.8331369159176246,\n",
       " 'eval_pearson_manhattan': 0.8263263709233586,\n",
       " 'eval_spearman_manhattan': 0.8327156016896692,\n",
       " 'eval_pearson_euclidean': 0.8269972815899188,\n",
       " 'eval_spearman_euclidean': 0.8331352218673969,\n",
       " 'eval_pearson_dot': 0.8282746052892088,\n",
       " 'eval_spearman_dot': 0.8331339952185075}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "ko_sbert_multitask= PLM_MODEL(ko_sbert_multitask)\n",
    "evaluation(data, ko_sbert_multitask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "msbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/17/2022 19:54:21 - INFO - __main__ -   Use `cls_before_pooler` for DiffCSE models. If you want to use other pooling policy, specify `pooler` argument.\n",
      "100%|██████████| 688/688 [00:09<00:00, 71.01it/s]\n",
      "100%|██████████| 688/688 [00:08<00:00, 85.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_pearson_cosine': 0.7344746573351436,\n",
       " 'eval_spearman_cosine': 0.7437088820329356,\n",
       " 'eval_pearson_manhattan': 0.7513232675402886,\n",
       " 'eval_spearman_manhattan': 0.7471425832200496,\n",
       " 'eval_pearson_euclidean': 0.7480378062050028,\n",
       " 'eval_spearman_euclidean': 0.7437068734137403,\n",
       " 'eval_pearson_dot': 0.7344746304708472,\n",
       " 'eval_spearman_dot': 0.7437101591465074}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "msbert= PLM_MODEL(msbert)\n",
    "evaluation(data, msbert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kcelectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at beomi/KcELECTRA-base were not used when initializing ElectraModel: ['discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias', 'discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "08/17/2022 19:55:35 - INFO - __main__ -   Use `cls_before_pooler` for DiffCSE models. If you want to use other pooling policy, specify `pooler` argument.\n",
      "100%|██████████| 688/688 [00:11<00:00, 59.86it/s]\n",
      "100%|██████████| 688/688 [00:12<00:00, 56.45it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_pearson_cosine': 0.14947429068230578,\n",
       " 'eval_spearman_cosine': 0.22748011671833054,\n",
       " 'eval_pearson_manhattan': 0.19911186599993233,\n",
       " 'eval_spearman_manhattan': 0.22932989363658562,\n",
       " 'eval_pearson_euclidean': 0.1990849249670487,\n",
       " 'eval_spearman_euclidean': 0.22747870750095237,\n",
       " 'eval_pearson_dot': 0.1494743415402462,\n",
       " 'eval_spearman_dot': 0.22747942966845075}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "kcelectra= PLM_MODEL(kcelectra)\n",
    "evaluation(data, kcelectra)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kosimcse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08/17/2022 19:56:39 - INFO - __main__ -   Use `cls_before_pooler` for DiffCSE models. If you want to use other pooling policy, specify `pooler` argument.\n",
      "100%|██████████| 688/688 [00:10<00:00, 63.62it/s]\n",
      "100%|██████████| 688/688 [00:12<00:00, 55.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_pearson_cosine': 0.8318715147053124,\n",
       " 'eval_spearman_cosine': 0.8348851282018802,\n",
       " 'eval_pearson_manhattan': 0.8300508476463244,\n",
       " 'eval_spearman_manhattan': 0.8346963758335499,\n",
       " 'eval_pearson_euclidean': 0.8301738518983001,\n",
       " 'eval_spearman_euclidean': 0.8348824759936001,\n",
       " 'eval_pearson_dot': 0.8318715139464892,\n",
       " 'eval_spearman_dot': 0.8348828650876597}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "kosimcse= PLM_MODEL(kosimcse)\n",
    "evaluation(data, kosimcse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kodiffcse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /home/keonwoo/anaconda3/envs/KoDiffCSE/sroberta_change_lr were not used when initializing RobertaModel: ['discriminator.encoder.layer.0.attention.self.value.bias', 'discriminator.encoder.layer.10.attention.self.key.weight', 'discriminator.encoder.layer.1.output.LayerNorm.weight', 'discriminator.encoder.layer.5.attention.self.value.weight', 'generator.roberta.encoder.layer.1.attention.self.key.bias', 'generator.roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.11.attention.self.key.bias', 'discriminator.embeddings.LayerNorm.weight', 'discriminator.encoder.layer.10.output.LayerNorm.bias', 'discriminator.encoder.layer.1.attention.self.query.bias', 'discriminator.encoder.layer.9.intermediate.dense.weight', 'discriminator.encoder.layer.5.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.7.attention.self.key.bias', 'discriminator.encoder.layer.0.attention.self.query.weight', 'generator.roberta.encoder.layer.2.output.LayerNorm.weight', 'generator.roberta.encoder.layer.4.attention.self.value.bias', 'generator.roberta.encoder.layer.5.output.dense.bias', 'discriminator.encoder.layer.0.output.LayerNorm.bias', 'generator.lm_head.bias', 'mlp.net.4.running_var', 'generator.roberta.encoder.layer.1.attention.self.value.weight', 'discriminator.encoder.layer.5.output.dense.bias', 'discriminator.encoder.layer.8.attention.self.key.weight', 'discriminator.encoder.layer.7.output.dense.bias', 'generator.roberta.encoder.layer.4.output.dense.bias', 'discriminator.encoder.layer.11.attention.self.value.weight', 'generator.roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.4.attention.self.key.weight', 'discriminator.encoder.layer.9.attention.self.key.bias', 'generator.roberta.embeddings.word_embeddings.weight', 'discriminator.encoder.layer.9.attention.self.query.weight', 'generator.roberta.encoder.layer.3.attention.self.value.bias', 'generator.roberta.encoder.layer.2.output.dense.weight', 'generator.roberta.embeddings.token_type_embeddings.weight', 'lm_head.dense.bias', 'discriminator.encoder.layer.3.output.dense.weight', 'discriminator.encoder.layer.9.attention.output.dense.weight', 'discriminator.encoder.layer.3.attention.self.query.weight', 'discriminator.encoder.layer.2.intermediate.dense.bias', 'discriminator.encoder.layer.0.intermediate.dense.weight', 'generator.lm_head.dense.bias', 'generator.roberta.encoder.layer.0.attention.output.dense.weight', 'generator.roberta.encoder.layer.0.intermediate.dense.bias', 'discriminator.encoder.layer.11.intermediate.dense.bias', 'discriminator.encoder.layer.11.attention.output.dense.bias', 'mlp.net.1.num_batches_tracked', 'discriminator.encoder.layer.10.output.LayerNorm.weight', 'generator.roberta.encoder.layer.0.attention.output.dense.bias', 'discriminator.encoder.layer.7.output.dense.weight', 'generator.roberta.encoder.layer.0.attention.self.key.bias', 'generator.roberta.encoder.layer.4.intermediate.dense.bias', 'generator.roberta.encoder.layer.0.intermediate.dense.weight', 'discriminator.encoder.layer.2.attention.self.value.weight', 'discriminator.encoder.layer.5.output.LayerNorm.weight', 'generator.lm_head.decoder.bias', 'discriminator.encoder.layer.9.attention.self.key.weight', 'discriminator.encoder.layer.7.attention.self.query.bias', 'generator.roberta.encoder.layer.5.attention.output.dense.bias', 'discriminator.encoder.layer.0.attention.output.dense.weight', 'discriminator.encoder.layer.6.attention.output.dense.bias', 'generator.roberta.encoder.layer.1.output.LayerNorm.weight', 'discriminator.encoder.layer.4.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.1.intermediate.dense.bias', 'lm_head.layer_norm.weight', 'generator.roberta.encoder.layer.1.attention.self.key.weight', 'generator.roberta.encoder.layer.1.output.LayerNorm.bias', 'generator.roberta.encoder.layer.4.output.LayerNorm.weight', 'generator.roberta.encoder.layer.0.attention.self.query.weight', 'discriminator.encoder.layer.2.output.dense.bias', 'discriminator.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.4.output.LayerNorm.bias', 'discriminator.encoder.layer.10.output.dense.bias', 'discriminator.encoder.layer.0.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.2.attention.self.value.bias', 'discriminator.encoder.layer.11.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.5.attention.self.key.bias', 'discriminator.encoder.layer.2.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.4.output.dense.weight', 'discriminator.embeddings.position_embeddings.weight', 'generator.roberta.encoder.layer.3.output.dense.bias', 'generator.roberta.encoder.layer.5.intermediate.dense.bias', 'discriminator.encoder.layer.3.attention.self.key.bias', 'discriminator.encoder.layer.4.intermediate.dense.bias', 'generator.roberta.encoder.layer.2.intermediate.dense.bias', 'mlp.net.0.weight', 'generator.roberta.encoder.layer.3.attention.self.value.weight', 'generator.roberta.encoder.layer.4.attention.self.query.bias', 'discriminator.encoder.layer.0.output.dense.bias', 'generator.roberta.encoder.layer.0.attention.self.value.weight', 'generator.roberta.encoder.layer.1.intermediate.dense.bias', 'generator.roberta.encoder.layer.1.attention.output.dense.weight', 'generator.roberta.encoder.layer.1.output.dense.weight', 'discriminator.encoder.layer.1.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.6.attention.self.value.bias', 'generator.roberta.encoder.layer.2.attention.self.value.weight', 'generator.roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.8.attention.self.key.bias', 'discriminator.encoder.layer.3.attention.output.dense.weight', 'discriminator.encoder.layer.10.attention.self.key.bias', 'generator.roberta.encoder.layer.5.attention.output.dense.weight', 'generator.roberta.encoder.layer.1.attention.self.query.bias', 'mlp.net.1.weight', 'discriminator.encoder.layer.7.attention.self.key.weight', 'discriminator.encoder.layer.0.intermediate.dense.bias', 'generator.roberta.encoder.layer.3.output.dense.weight', 'discriminator.encoder.layer.5.attention.self.query.bias', 'discriminator.encoder.layer.10.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.7.output.LayerNorm.weight', 'generator.roberta.encoder.layer.0.output.dense.bias', 'discriminator.encoder.layer.4.output.LayerNorm.bias', 'discriminator.encoder.layer.10.attention.self.value.bias', 'generator.lm_head.layer_norm.bias', 'generator.roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'generator.roberta.embeddings.position_ids', 'generator.roberta.encoder.layer.4.attention.self.query.weight', 'generator.roberta.encoder.layer.3.attention.self.key.bias', 'generator.lm_head.dense.weight', 'discriminator.encoder.layer.10.intermediate.dense.bias', 'discriminator.encoder.layer.7.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.11.output.LayerNorm.weight', 'discriminator.encoder.layer.10.output.dense.weight', 'discriminator.encoder.layer.11.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.9.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.10.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.2.attention.output.dense.bias', 'discriminator.encoder.layer.11.attention.self.key.weight', 'discriminator.encoder.layer.5.attention.self.value.bias', 'discriminator.encoder.layer.11.intermediate.dense.weight', 'discriminator.encoder.layer.3.intermediate.dense.bias', 'discriminator.encoder.layer.3.output.LayerNorm.weight', 'generator.roberta.encoder.layer.1.output.dense.bias', 'discriminator.encoder.layer.1.output.dense.bias', 'discriminator.encoder.layer.1.attention.self.value.weight', 'discriminator.encoder.layer.0.output.LayerNorm.weight', 'mlp.net.1.running_var', 'discriminator.encoder.layer.8.attention.output.dense.weight', 'generator.roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.11.output.dense.weight', 'discriminator.encoder.layer.6.output.dense.weight', 'discriminator.encoder.layer.3.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.10.intermediate.dense.weight', 'generator.roberta.encoder.layer.0.output.LayerNorm.bias', 'generator.roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.2.attention.output.dense.bias', 'discriminator.encoder.layer.1.attention.self.query.weight', 'generator.roberta.encoder.layer.3.intermediate.dense.weight', 'lm_head.decoder.bias', 'discriminator.encoder.layer.1.intermediate.dense.weight', 'discriminator.encoder.layer.9.attention.output.dense.bias', 'generator.roberta.encoder.layer.2.attention.self.query.bias', 'discriminator.encoder.layer.6.attention.self.key.bias', 'discriminator.encoder.layer.2.attention.self.key.bias', 'generator.roberta.encoder.layer.0.output.dense.weight', 'discriminator.encoder.layer.9.intermediate.dense.bias', 'generator.roberta.encoder.layer.0.attention.self.value.bias', 'discriminator.encoder.layer.9.output.dense.weight', 'generator.roberta.encoder.layer.2.attention.self.query.weight', 'generator.roberta.encoder.layer.0.output.LayerNorm.weight', 'generator.roberta.encoder.layer.1.attention.self.query.weight', 'generator.roberta.encoder.layer.3.attention.output.dense.bias', 'discriminator.encoder.layer.7.output.LayerNorm.bias', 'generator.roberta.encoder.layer.4.attention.output.dense.bias', 'discriminator.encoder.layer.10.attention.self.value.weight', 'generator.roberta.encoder.layer.5.output.dense.weight', 'discriminator.encoder.layer.1.attention.output.dense.bias', 'discriminator.encoder.layer.7.intermediate.dense.weight', 'discriminator.encoder.layer.5.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.6.attention.self.value.weight', 'discriminator.embeddings.LayerNorm.bias', 'discriminator.encoder.layer.4.output.dense.bias', 'discriminator.encoder.layer.1.output.dense.weight', 'discriminator.encoder.layer.4.intermediate.dense.weight', 'discriminator.encoder.layer.11.attention.self.value.bias', 'discriminator.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.2.output.LayerNorm.bias', 'lm_head.decoder.weight', 'discriminator.encoder.layer.4.attention.self.key.bias', 'discriminator.encoder.layer.6.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.11.attention.self.query.weight', 'discriminator.encoder.layer.4.attention.self.query.bias', 'discriminator.encoder.layer.6.attention.self.query.weight', 'discriminator.encoder.layer.2.output.dense.weight', 'generator.roberta.encoder.layer.0.attention.self.query.bias', 'discriminator.encoder.layer.5.attention.self.query.weight', 'discriminator.encoder.layer.6.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.5.attention.output.dense.bias', 'mlp.net.1.running_mean', 'discriminator.encoder.layer.3.attention.self.value.bias', 'generator.roberta.encoder.layer.3.attention.self.query.bias', 'generator.roberta.encoder.layer.2.attention.self.key.weight', 'discriminator.encoder.layer.8.attention.self.query.weight', 'discriminator.encoder.layer.1.attention.self.value.bias', 'discriminator.encoder.layer.9.output.LayerNorm.weight', 'discriminator.encoder.layer.11.attention.self.query.bias', 'discriminator.encoder.layer.0.attention.self.key.bias', 'generator.roberta.encoder.layer.0.attention.self.key.weight', 'generator.roberta.encoder.layer.3.intermediate.dense.bias', 'generator.roberta.encoder.layer.2.attention.self.key.bias', 'discriminator.encoder.layer.7.attention.output.dense.weight', 'discriminator.encoder.layer.7.intermediate.dense.bias', 'discriminator.encoder.layer.3.attention.self.key.weight', 'discriminator.encoder.layer.2.intermediate.dense.weight', 'generator.roberta.encoder.layer.2.attention.output.dense.weight', 'discriminator.encoder.layer.9.output.LayerNorm.bias', 'discriminator.encoder.layer.4.attention.self.query.weight', 'discriminator.encoder.layer.8.attention.self.query.bias', 'discriminator.encoder.layer.3.attention.output.LayerNorm.bias', 'discriminator.embeddings.token_type_embeddings.weight', 'generator.roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.5.attention.self.query.bias', 'discriminator.encoder.layer.8.attention.output.dense.bias', 'discriminator.encoder.layer.10.attention.self.query.bias', 'discriminator.encoder.layer.1.attention.self.key.bias', 'generator.roberta.encoder.layer.1.attention.output.dense.bias', 'discriminator.encoder.layer.4.attention.self.value.weight', 'generator.roberta.encoder.layer.2.output.dense.bias', 'generator.roberta.encoder.layer.5.output.LayerNorm.bias', 'discriminator.embeddings.word_embeddings.weight', 'mlp.net.4.num_batches_tracked', 'generator.lm_head.decoder.weight', 'discriminator.encoder.layer.8.intermediate.dense.bias', 'discriminator.encoder.layer.11.output.dense.bias', 'generator.roberta.embeddings.LayerNorm.bias', 'discriminator.encoder.layer.3.attention.self.value.weight', 'discriminator.encoder.layer.0.output.dense.weight', 'discriminator.encoder.layer.8.output.LayerNorm.bias', 'discriminator.encoder.layer.8.intermediate.dense.weight', 'discriminator.encoder.layer.8.output.dense.weight', 'discriminator.encoder.layer.8.output.LayerNorm.weight', 'discriminator.encoder.layer.6.intermediate.dense.weight', 'discriminator.encoder.layer.0.attention.output.dense.bias', 'discriminator.encoder.layer.5.output.LayerNorm.bias', 'generator.roberta.encoder.layer.4.attention.self.key.bias', 'lm_head.bias', 'discriminator.encoder.layer.8.attention.self.value.weight', 'discriminator.encoder.layer.0.attention.self.key.weight', 'generator.roberta.encoder.layer.1.intermediate.dense.weight', 'discriminator.encoder.layer.9.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.5.intermediate.dense.bias', 'discriminator.encoder.layer.1.output.LayerNorm.bias', 'generator.roberta.encoder.layer.2.intermediate.dense.weight', 'discriminator.encoder.layer.10.attention.output.dense.bias', 'mlp.net.4.running_mean', 'discriminator.encoder.layer.9.attention.self.value.bias', 'discriminator.embeddings.position_ids', 'generator.lm_head.layer_norm.weight', 'discriminator.encoder.layer.5.output.dense.weight', 'discriminator.encoder.layer.7.attention.self.query.weight', 'discriminator.encoder.layer.2.attention.self.key.weight', 'mlp.net.1.bias', 'discriminator.encoder.layer.4.attention.self.value.bias', 'discriminator.encoder.layer.0.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.3.output.LayerNorm.bias', 'discriminator.encoder.layer.11.attention.output.dense.weight', 'lm_head.layer_norm.bias', 'discriminator.encoder.layer.6.output.LayerNorm.weight', 'discriminator.encoder.layer.6.output.LayerNorm.bias', 'discriminator.encoder.layer.0.attention.self.value.weight', 'discriminator.encoder.layer.2.attention.self.query.weight', 'discriminator.encoder.layer.7.attention.self.value.weight', 'generator.roberta.encoder.layer.5.attention.self.value.bias', 'generator.roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.9.output.dense.bias', 'generator.roberta.encoder.layer.3.attention.self.query.weight', 'generator.roberta.encoder.layer.3.output.LayerNorm.weight', 'discriminator.encoder.layer.4.attention.self.key.weight', 'discriminator.encoder.layer.3.attention.output.dense.bias', 'generator.roberta.encoder.layer.5.output.LayerNorm.weight', 'generator.roberta.encoder.layer.1.attention.self.value.bias', 'discriminator.encoder.layer.4.output.LayerNorm.weight', 'discriminator.encoder.layer.6.intermediate.dense.bias', 'lm_head.dense.weight', 'generator.roberta.embeddings.LayerNorm.weight', 'discriminator.encoder.layer.6.output.dense.bias', 'discriminator.encoder.layer.10.attention.self.query.weight', 'generator.roberta.encoder.layer.3.output.LayerNorm.bias', 'generator.roberta.encoder.layer.5.attention.self.query.weight', 'discriminator.encoder.layer.7.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.2.attention.self.query.bias', 'discriminator.encoder.layer.5.attention.self.key.weight', 'discriminator.encoder.layer.2.output.LayerNorm.bias', 'generator.roberta.embeddings.position_embeddings.weight', 'generator.roberta.encoder.layer.4.attention.output.dense.weight', 'discriminator.encoder.layer.10.attention.output.dense.weight', 'generator.roberta.encoder.layer.2.attention.self.value.bias', 'generator.roberta.encoder.layer.5.attention.self.key.weight', 'discriminator.encoder.layer.5.intermediate.dense.weight', 'generator.roberta.encoder.layer.3.attention.self.key.weight', 'discriminator.encoder.layer.6.attention.self.key.weight', 'discriminator.encoder.layer.7.attention.self.value.bias', 'discriminator.encoder.layer.4.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.4.attention.self.value.weight', 'generator.roberta.encoder.layer.5.attention.self.key.bias', 'discriminator.encoder.layer.3.intermediate.dense.weight', 'discriminator.encoder.layer.9.attention.self.value.weight', 'discriminator.encoder.layer.2.attention.output.dense.weight', 'discriminator.encoder.layer.6.attention.output.dense.weight', 'discriminator.encoder.layer.8.attention.output.LayerNorm.weight', 'generator.roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'discriminator.encoder.layer.7.attention.output.dense.bias', 'generator.roberta.encoder.layer.4.intermediate.dense.weight', 'discriminator.encoder.layer.4.output.dense.weight', 'discriminator.encoder.layer.6.attention.self.query.bias', 'discriminator.encoder.layer.2.output.LayerNorm.weight', 'generator.roberta.encoder.layer.5.intermediate.dense.weight', 'discriminator.encoder.layer.8.attention.output.LayerNorm.bias', 'discriminator.encoder.layer.8.output.dense.bias', 'generator.roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'generator.roberta.encoder.layer.3.attention.output.dense.weight', 'discriminator.encoder.layer.5.attention.output.dense.weight', 'discriminator.encoder.layer.11.output.LayerNorm.bias', 'generator.roberta.encoder.layer.5.attention.self.value.weight', 'discriminator.encoder.layer.3.output.dense.bias', 'discriminator.encoder.layer.1.attention.self.key.weight', 'mlp.net.3.weight', 'discriminator.encoder.layer.3.attention.self.query.bias', 'discriminator.encoder.layer.4.attention.output.dense.bias', 'discriminator.encoder.layer.0.attention.self.query.bias', 'discriminator.encoder.layer.8.attention.self.value.bias', 'discriminator.encoder.layer.9.attention.self.query.bias', 'discriminator.encoder.layer.1.attention.output.dense.weight', 'discriminator.encoder.layer.4.attention.output.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at /home/keonwoo/anaconda3/envs/KoDiffCSE/sroberta_change_lr and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "08/17/2022 19:57:37 - INFO - __main__ -   Use `cls_before_pooler` for DiffCSE models. If you want to use other pooling policy, specify `pooler` argument.\n",
      "100%|██████████| 688/688 [00:08<00:00, 84.39it/s]\n",
      "100%|██████████| 688/688 [00:11<00:00, 60.28it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_pearson_cosine': 0.839635876989272,\n",
       " 'eval_spearman_cosine': 0.8445370018525205,\n",
       " 'eval_pearson_manhattan': 0.8422651782455157,\n",
       " 'eval_spearman_manhattan': 0.8445933927701841,\n",
       " 'eval_pearson_euclidean': 0.842234823517936,\n",
       " 'eval_spearman_euclidean': 0.8445351939944561,\n",
       " 'eval_pearson_dot': 0.8396358754378573,\n",
       " 'eval_spearman_dot': 0.8445377247537453}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "kodiffcse= PLM_MODEL(kodiffcse)\n",
    "evaluation(data, kodiffcse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kobert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
      "The class this function is called from is 'KoBertTokenizer'.\n",
      "08/17/2022 19:59:18 - INFO - __main__ -   Use `cls_before_pooler` for DiffCSE models. If you want to use other pooling policy, specify `pooler` argument.\n",
      "100%|██████████| 688/688 [00:13<00:00, 50.61it/s]\n",
      "100%|██████████| 688/688 [00:10<00:00, 64.96it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_pearson_cosine': 0.21849038706847992,\n",
       " 'eval_spearman_cosine': 0.25617634499135167,\n",
       " 'eval_pearson_manhattan': 0.26066839293480387,\n",
       " 'eval_spearman_manhattan': 0.2542042615476074,\n",
       " 'eval_pearson_euclidean': 0.25948163369055666,\n",
       " 'eval_spearman_euclidean': 0.25617530966095187,\n",
       " 'eval_pearson_dot': 0.2184903961732708,\n",
       " 'eval_spearman_dot': 0.2561754521316946}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "The Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "kobert= PLM_MODEL(kobert)\n",
    "evaluation(data, kobert)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "ckpt = 'princeton-nlp/sup-simcse-bert-base-uncased'\n",
    "model = AutoModel.from_pretrained(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertModel\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.bert.modeling_bert.BertModel"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.auto.modeling_auto.AutoModel"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('bgmRS': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "97f12e04903685b25708d91ecd1d4aa07ab7b586436cb43ce5df299ed23dfd1d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
